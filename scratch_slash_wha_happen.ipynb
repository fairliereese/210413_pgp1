{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "drawn-johnson",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fairliereese/miniconda3/lib/python3.7/site-packages/anndata/_core/anndata.py:21: FutureWarning: pandas.core.index is deprecated and will be removed in a future version.  The public classes are available in the top-level namespace.\n",
      "  from pandas.core.index import RangeIndex\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scanpy as sc\n",
    "import numpy as np\n",
    "import anndata\n",
    "import scipy.stats as st\n",
    "import statsmodels.stats as stm\n",
    "from statsmodels.stats.multitest import multipletests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "secondary-infection",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wat da fuq\n"
     ]
    }
   ],
   "source": [
    "print('wat da fuq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "printable-bedroom",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hewwo?\n",
      "hewwo 2\n",
      "hewwo3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fairliereese/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/Users/fairliereese/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hewwo5\n",
      "hewwo6\n",
      "18049\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "# filter a list of TSSs for each gene\n",
    "annot = 'talon/pgp1_talon_read_annot.tsv'\n",
    "tss_reads = 'tss_reads.bed'\n",
    "\n",
    "df = pd.read_csv(annot, sep='\\t')\n",
    "\n",
    "print('hewwo?')\n",
    "\n",
    "# remove sirvs and erccs\n",
    "df = df.loc[~df.chrom.str.contains('SIRV')]\n",
    "df = df.loc[~df.chrom.str.contains('ERCC')]\n",
    "\n",
    "# print('Before assigning each read a TSS')\n",
    "# print(len(df.index))\n",
    "ends = pd.read_csv(tss_reads, sep='\\t', header=None,\n",
    "            names=['chrom', 'start', 'end', 'read_name', 'tss_id', 'stand'])\n",
    "\n",
    "# merge on read name\n",
    "df = df.merge(ends, how='inner', on='read_name')\n",
    "\n",
    "# groupby on gene and tss\n",
    "df = df[['read_name', 'annot_gene_name', 'tss_id']].groupby(['annot_gene_name', 'tss_id']).count()\n",
    "\n",
    "print('hewwo 2')\n",
    "\n",
    "# filter tsss for those that have >10% of the reads\n",
    "# for the most highly-expressed tss of the gene\n",
    "df.reset_index(inplace=True)\n",
    "df.rename({'read_name':'count'}, axis=1, inplace=True)\n",
    "temp = df.loc[df.apply(lambda x: x['count'] >= df.loc[df.annot_gene_name==x.annot_gene_name, 'count'].max()*0.1, axis=1)]\n",
    "\n",
    "print('hewwo3')\n",
    "\n",
    "#, assign each TSS a name, and quantify TSS exp\n",
    "temp.sort_values(by=['annot_gene_name'], inplace=True)\n",
    "temp['tss_id_2'] = np.nan\n",
    "prev_gene = None\n",
    "for ind, entry in temp.iterrows():\n",
    "    curr_gene = entry.annot_gene_name\n",
    "    if curr_gene != prev_gene:\n",
    "        i = 1\n",
    "    else:\n",
    "        i += 1\n",
    "    prev_gene = curr_gene\n",
    "    temp.loc[ind, 'tss_id_2'] = '{}_{}'.format(curr_gene, i)\n",
    "    \n",
    "# merged called TSSs back in with read annot\n",
    "df = pd.read_csv(annot, sep='\\t')\n",
    "ends = pd.read_csv(tss_reads, sep='\\t', header=None,\n",
    "            names=['chrom', 'start', 'end', 'read_name', 'tss_id', 'stand'])\n",
    "ends = ends[['read_name', 'tss_id']]\n",
    "\n",
    "print('hewwo5')\n",
    "\n",
    "# merge on read name\n",
    "df = df.merge(ends, how='inner', on='read_name')\n",
    "temp = temp[['annot_gene_name', 'tss_id', 'tss_id_2']]\n",
    "df = df.merge(temp, how='inner', on=['annot_gene_name', 'tss_id'])\n",
    "\n",
    "# format like talon ab\n",
    "cols = ['annot_gene_name', 'annot_gene_id', 'dataset', 'tss_id_2']\n",
    "df = df[cols+['read_name']].groupby(cols).count().reset_index()\n",
    "df.rename({'read_name':'counts', 'tss_id_2':'tss_id'}, axis=1, inplace=True)\n",
    "df = df.pivot(index=['annot_gene_name', 'annot_gene_id', 'tss_id'], columns='dataset', values='counts')\n",
    "df.reset_index(inplace=True)\n",
    "df = df.rename_axis(None, axis=1)\n",
    "df.fillna(0, inplace=True)\n",
    "\n",
    "print('hewwo6')\n",
    "\n",
    "# i guess sum over genes with the same name smh\n",
    "df.drop('annot_gene_id', axis=1, inplace=True)\n",
    "df = df.groupby(['annot_gene_name', 'tss_id']).sum().reset_index()\n",
    "\n",
    "print(len(df.index))\n",
    "print(len(df.columns))\n",
    "df.to_csv('talon/pgp1_tss_talon_abundance.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "reliable-daughter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df: talon abundance file (either tss or transcript)\n",
    "# cond_map: dictionary of {condition: [dataset1, dataset2]}; how you want to group datasets\n",
    "# how: whether to make a tss or iso level adata; 'tss' or 'iso'\n",
    "# pass_list: if 'iso', file of valid transcript IDs that pass filtering\n",
    "def make_adata(df, cond_map, how='iso', pass_list=None):\n",
    "    \n",
    "    # filter talon ab file based on pass list\n",
    "#     df = pd.read_csv(ab_file, sep='\\t')\n",
    "    if pass_list:\n",
    "        pass_list = pd.read_csv(pass_list, header=None, names=['gene_id', 'transcript_id'])\n",
    "        df = df.loc[df.transcript_ID.isin(pass_list.transcript_id.tolist())]\n",
    "\n",
    "    # obs table\n",
    "    obs = pd.DataFrame.from_dict(cond_map, orient='index')\n",
    "    obs.reset_index(inplace=True)\n",
    "    id_vars = ['index']\n",
    "    value_vars = obs.columns[1:]\n",
    "    obs = obs.melt(id_vars=id_vars, value_vars=value_vars)\n",
    "    obs.drop('variable', axis=1, inplace=True)\n",
    "    obs.rename({'index':'condition', 'value':'dataset'}, axis=1, inplace=True)\n",
    "\n",
    "    # var table\n",
    "    if how=='iso':\n",
    "        var_cols = ['annot_transcript_name', 'annot_gene_name',\\\n",
    "                    'annot_transcript_id', 'annot_gene_id', \\\n",
    "                    'gene_ID', 'transcript_ID', 'transcript_novelty', \\\n",
    "                    'ISM_subtype']\n",
    "        var = df[var_cols]\n",
    "        var.rename({'transcript_ID':'transcript_id', \\\n",
    "                    'gene_ID':'gene_id',\\\n",
    "                    'annot_gene_name': 'gene_name'}, axis=1, inplace=True)\n",
    "    if how=='tss': \n",
    "        var_cols = ['annot_gene_name', 'tss_id']\n",
    "        var = df[var_cols]\n",
    "        var.rename({'annot_gene_name': 'gene_name'}, axis=1, inplace=True)\n",
    "        \n",
    "    # X table\n",
    "    df = df.transpose()\n",
    "    df = df.loc[df.index.isin(obs.dataset.tolist())]\n",
    "    obs_order = obs['dataset'].reset_index().set_index('dataset')\n",
    "    df['dataset_num'] = df.index.map(obs_order['index'])\n",
    "    df.sort_values('dataset_num', inplace=True)\n",
    "    df.drop('dataset_num', axis=1, inplace=True)\n",
    "    X = df.to_numpy()\n",
    "\n",
    "    adata = anndata.AnnData(obs=obs, var=var, X=X) \n",
    "    \n",
    "    return adata\n",
    "\n",
    "# gene_df: pandas dataframe with expression values in each condition for each TSS in a gene\n",
    "# conditions: list of str of condition names\n",
    "# rc: threshold of read count per gene in each condition necessary to test this gene\n",
    "def test_gene(gene_df, conditions, col, id_col, rc=10):\n",
    "    \n",
    "    gene_df = gene_df.pivot(index=col, columns=id_col, values='counts')\n",
    "    gene_df = gene_df.transpose()\n",
    "    \n",
    "    groups = gene_df.columns.tolist()\n",
    "    gene_df['total_counts'] = gene_df[groups].sum(axis=1)\n",
    "    gene_df.sort_values(by='total_counts', ascending=False, inplace=True)\n",
    "\n",
    "    if len(gene_df.index) > 11:\n",
    "        gene_df.reset_index(inplace=True)\n",
    "\n",
    "        beep = gene_df.iloc[10:].sum()\n",
    "        beep[id_col] = 'all_other'\n",
    "        beep.index.name = None  \n",
    "        beep = pd.DataFrame(beep).transpose()\n",
    "\n",
    "        gene_df = gene_df.iloc[:10]\n",
    "        gene_df = pd.concat([gene_df, beep])  \n",
    "        \n",
    "    # limit to just isoforms with > 0 expression in both conditions\n",
    "    cond1 = conditions[0]\n",
    "    cond2 = conditions[1]\n",
    "    gene_df = gene_df.loc[(gene_df[cond1]>0)&(gene_df[cond2]>0)]\n",
    "    \n",
    "    # does this gene reach the desired read count threshold?\n",
    "    for cond in conditions:\n",
    "        if gene_df[cond].sum() < rc:\n",
    "            return np.nan, np.nan\n",
    "    \n",
    "    # only do the rest if there's nothing left\n",
    "    if gene_df.empty:\n",
    "        return np.nan, np.nan\n",
    "    \n",
    "    # calculate the percent of each sample each TSS accounts for\n",
    "    cond_pis = []\n",
    "    for cond in conditions:\n",
    "        total_col = '{}_total'.format(cond)\n",
    "        pi_col = '{}_pi'.format(cond)\n",
    "        total_count = gene_df[cond].sum()\n",
    "\n",
    "        cond_pis.append(pi_col)\n",
    "\n",
    "        gene_df[total_col] = total_count\n",
    "        gene_df[pi_col] = (gene_df[cond]/gene_df[total_col])*100\n",
    "        \n",
    "    # compute isoform-level and gene-level delta pis\n",
    "    gene_df['dpi'] = gene_df[cond_pis[0]] - gene_df[cond_pis[1]]\n",
    "    gene_df['abs_dpi'] = gene_df.dpi.abs()\n",
    "    gene_dpi = gene_df.iloc[:2].abs_dpi.sum()    \n",
    "    \n",
    "    # chi squared test \n",
    "    chi_table = gene_df[conditions].to_numpy()\n",
    "    chi2, p, dof, exp = st.chi2_contingency(chi_table)\n",
    "    \n",
    "    return p, gene_dpi\n",
    "\n",
    "def filter_die_results(df, p, dpi):\n",
    "    df = df.loc[(df.adj_p_val<=p)&(df.dpi>=dpi)]\n",
    "    return df\n",
    "\n",
    "# adata: adata with TSS or iso expression \n",
    "# conditions: len 2 list of strings of conditions to compare\n",
    "# col: string, which column the condition labels are in\n",
    "# how: 'tss' or 'iso'\n",
    "def get_die(adata, conditions, how='tss', rc=15):\n",
    "    \n",
    "    if how == 'tss':\n",
    "        id_col = 'tss_id'\n",
    "    elif how == 'iso':\n",
    "        id_col = 'transcript_id'\n",
    "    \n",
    "    # make df that we can groupby\n",
    "    col = 'condition'\n",
    "    colnames = adata.var[id_col].tolist()\n",
    "    rownames = adata.obs.dataset.tolist()    \n",
    "    raw = adata.X\n",
    "    df = pd.DataFrame(data=raw, index=rownames, columns=colnames)\n",
    "    df.reset_index(inplace=True)\n",
    "    df.rename({'index':'dataset'}, axis=1, inplace=True)\n",
    "    samp = adata.obs[['dataset', col]]\n",
    "    df = df.merge(samp, how='left', on='dataset')\n",
    "    \n",
    "    # limit to only the samples that we want in this condition\n",
    "    df[col] = df[col].astype('str')\n",
    "    df = df.loc[df[col].isin(conditions)]\n",
    "        \n",
    "    # groupby sample type and sum over gen\n",
    "    df.drop('dataset', axis=1, inplace=True)\n",
    "    df = df.groupby(col).sum().reset_index()\n",
    "    \n",
    "    # melty boi\n",
    "    tss_cols = df.columns.tolist()[1:]\n",
    "    df = df.melt(id_vars=col, value_vars=tss_cols)\n",
    "    \n",
    "    # rename some cols\n",
    "    df.rename({'variable':id_col,'value':'counts'}, axis=1, inplace=True)\n",
    "    \n",
    "    # merge with gene names\n",
    "    df = df.merge(adata.var, how='left', on=id_col)\n",
    "    \n",
    "    # get total number of tss or iso / gene\n",
    "    bop = df[['gene_name', id_col]].groupby('gene_name').count().reset_index()\n",
    "    \n",
    "    # construct tables for each gene and test!\n",
    "    gene_names = df.gene_name.unique().tolist()\n",
    "    gene_de_df = pd.DataFrame(index=gene_names, columns=['p_val', 'dpi'], data=[[np.nan for i in range(2)] for j in range(len(gene_names))])\n",
    "    for gene in gene_names:\n",
    "        gene_df = df.loc[df.gene_name==gene]\n",
    "        p, dpi = test_gene(gene_df, conditions, col, id_col, rc=rc)\n",
    "        gene_de_df.loc[gene, 'p_val'] = p\n",
    "        gene_de_df.loc[gene, 'dpi'] = dpi\n",
    "        \n",
    "    # correct p values \n",
    "    gene_de_df.dropna(axis=0, inplace=True)\n",
    "    p_vals = gene_de_df.p_val.tolist()\n",
    "    _, adj_p_vals, _, _ = multipletests(p_vals, method='fdr_bh')\n",
    "    gene_de_df['adj_p_val'] = adj_p_vals\n",
    "    \n",
    "    gene_de_df.reset_index(inplace=True)\n",
    "    \n",
    "        \n",
    "    return gene_de_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "still-blackberry",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fairliereese/miniconda3/lib/python3.7/site-packages/pandas/core/frame.py:4303: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n",
      "Transforming to str index.\n",
      "Transforming to str index.\n"
     ]
    }
   ],
   "source": [
    "ab_file = 'talon/pgp1_tss_talon_abundance.tsv'\n",
    "cond_map = {'Astrocytes': ['astro_1', 'astro_2'], \\\n",
    "            'Excitatory neurons': ['excite_neuron_1', 'excite_neuron_2'], \\\n",
    "            'PGP1': ['pgp1_1', 'pgp1_2']}\n",
    "\n",
    "df = pd.read_csv(ab_file, sep='\\t')\n",
    "adata = make_adata(df, cond_map, how='tss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "criminal-poison",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Astrocytes', 'Excitatory neurons')\n",
      "      index  p_val  dpi  adj_p_val\n",
      "0      A1BG    1.0  0.0        1.0\n",
      "1  A1BG-AS1    1.0  0.0        1.0\n",
      "2      AAAS    1.0  0.0        1.0\n",
      "3      AACS    1.0  0.0        1.0\n",
      "4      AAK1    1.0  0.0        1.0\n"
     ]
    }
   ],
   "source": [
    "how = 'tss'\n",
    "# do one test for each pair of conditions\n",
    "tested = []\n",
    "conditions = ['Astrocytes', 'Excitatory neurons', 'PGP1']\n",
    "for c1 in conditions:\n",
    "    for c2 in conditions: \n",
    "        if (c1, c2) in tested or c1 == c2 or (c2, c1) in tested:\n",
    "            continue\n",
    "        else:\n",
    "            tested.append((c1,c2))\n",
    "        print((c1, c2))\n",
    "        df = get_die(adata, [c1, c2], how=how, rc=10)\n",
    "        fname = '{}_{}_{}_die.tsv'.format(c1, c2, how)\n",
    "        print(df.head())\n",
    "        df.to_csv(fname, sep='\\t', index=False)\n",
    "        break\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "excellent-repeat",
   "metadata": {},
   "outputs": [],
   "source": [
    "tested = []\n",
    "p = 0.05\n",
    "dpi = 10\n",
    "conditions = ['Astrocytes', 'Excitatory neurons', 'PGP1']\n",
    "for c1 in conditions:\n",
    "    for c2 in conditions: \n",
    "        if (c1, c2) in tested or c1 == c2 or (c2, c1) in tested:\n",
    "            continue\n",
    "        else:\n",
    "            tested.append((c1,c2))\n",
    "        fname = '{}_{}_{}_die.tsv'.format(c1, c2, how)\n",
    "        df = pd.read_csv(fname, sep='\\t')\n",
    "        df = filter_die_results(df, p, dpi)\n",
    "        fname = '{}_{}_{}_sig_die.tsv'.format(c1, c2, how)\n",
    "        df.to_csv(fname, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comfortable-college",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "knowing-class",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "verified-liabilities",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shlex\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "composite-horizon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"input\"\n",
    "url = 'http://crick.bio.uci.edu/freese/210413_pgp1_hub/'\n",
    "c_dict = {'astro_1': '#f6ef7c', 'astro_2': '#eabc68',\\\n",
    "          'excite_neuron_1': '#e4d3cd', 'excite_neuron_2': '#d3a8b2',\\\n",
    "          'pgp1_1': '#bef4ff', 'pgp1_2': '#73a8b2'}\n",
    "config = 'talon/config.csv'\n",
    "genome = 'mm10'\n",
    "hub_name = 'pgp1'\n",
    "email = 'freese@uci.edu'\n",
    "\n",
    "genomefile = 'hub/genomes.txt'\n",
    "hubfile = 'hub/hub.txt'\n",
    "hubfile_relative = 'hub.txt'\n",
    "trackdb = 'hub/{}/trackDb.txt'.format(genome)\n",
    "relative_trackdb = '{}/trackDb.txt'.format(genome)\n",
    "relative_genome = 'genomes.txt'\n",
    "scp_location = 'freese@crick.bio.uci.edu:~/pub/210413_pgp1_hub/'\n",
    "try:\n",
    "    os.makedirs(os.path.dirname(ofile))\n",
    "except:\n",
    "    pass\n",
    "df = pd.read_csv(config, header=None, names=['sample', 'condition', \\\n",
    "                                             'platform', 'filepath'])\n",
    "df['url'] = df.apply(lambda x: url+x['sample']+'.bam', axis=1)\n",
    "df['local_loc'] = df.apply(lambda x: 'talon_tmp/'+x['sample']+'.bam', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "structured-tower",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = 'module load samtools'\n",
    "cmd = shlex.split(cmd)\n",
    "result = subprocess.run(cmd)\n",
    "for ind, entry in df.iterrows():\n",
    "    cmd = 'samtools index {}'.format(entry.local_loc)\n",
    "    cmd = shlex.split(cmd)\n",
    "    result = subprocess.run(cmd)  \n",
    "    \n",
    "    cmd = 'scp {}* {}'.format(entry.local_loc, scp_location)\n",
    "    cmd = shlex.split(cmd)\n",
    "    result = subprocess.run(cmd)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "earlier-creek",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bam_hub_entry(df, c_dict, ofile):\n",
    "    with open(ofile, 'w') as o:\n",
    "        for ind, e in df.iterrows():\n",
    "            c = c_dict[e['sample']]\n",
    "            c = c.lstrip('#')\n",
    "            c = tuple(int(c[i:i+2], 16) for i in (0, 2, 4))\n",
    "\n",
    "            s = 'track {}_reads\\n'.format(e['sample'])\n",
    "            s += 'bigDataUrl {}\\n'.format(e.url)\n",
    "            s += 'shortLabel {}_reads\\n'.format(e['sample'])\n",
    "            s += 'longLabel {}_reads\\n'.format(e['sample'])\n",
    "            s += 'type bam\\n'\n",
    "            s += 'visibility squish'\n",
    "            s += 'bamColorMode off\\n'\n",
    "            s += 'color {},{},{}\\n\\n'.format(c[0],c[1],c[2])\n",
    "            o.write(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "worth-hotel",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_bam_hub_entry(df, c_dict, trackdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "blank-aaron",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(genomefile, 'w') as o:\n",
    "    s = 'genome {}\\n'.format(genome)\n",
    "    s += 'trackDb {}\\n'.format(relative_trackdb)\n",
    "    o.write(s)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "patient-delay",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(hubfile, 'w') as o:\n",
    "    s = 'hub {}\\n'.format(hub_name)\n",
    "    s += 'shortLabel {}\\n'.format(hub_name)\n",
    "    s += 'longLabel {}\\n'.format(hub_name)\n",
    "    s += 'genomesFile {}\\n'.format(relative_genome)\n",
    "    s += 'email {}\\n'.format(email)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
